{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbXu1AVknF-M"
      },
      "source": [
        "Data citation : https://www.kaggle.com/code/mitishaagarwal/netflix-prize-data\n",
        "\n",
        "Original citation : https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data (Netflix Prize data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e_qGocnMae"
      },
      "source": [
        "# Neural Collaborative Filtering Recommendation System (Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itCOV0JRm2hT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('/content/datasets/train.csv')\n",
        "test_data = pd.read_csv('/content/datasets/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adOAEEvf2R9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21a9508-b0e8-45e6-96c6-2c215574b8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization took: 8.02 ms\n",
            "Type conversion took: 12.23 ms\n",
            "Parser memory cleanup took: 0.01 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17770, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Read the CSV file while skipping bad lines\n",
        "movie_titles = pd.read_csv(\"/content/datasets/movie_titles.csv\", sep=',', header = None,\n",
        "                           names=['movie_id', 'year_of_release', 'title', 'title_2', 'title_3', 'title_4'], verbose=True,\n",
        "                      index_col = 'movie_id', encoding = \"ISO-8859-1\")\n",
        "\n",
        "# Combine 'title', 'title_2', and 'title_3' columns into a single 'combined_title' column\n",
        "movie_titles['combined_title'] = movie_titles[['title', 'title_2', 'title_3', 'title_4']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop individual title columns to retain only the combined title column\n",
        "movie_titles.drop(columns=['title', 'title_2', 'title_3', 'title_4'], inplace=True)\n",
        "\n",
        "movie_titles = movie_titles.reset_index()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "movie_titles.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tAVcb-T2fBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d71c18c-9121-4a05-d29d-72231f2d735a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of train dataset :  (1048575, 4)\n",
            "Dimensions of test dataset :  (1048575, 4)\n"
          ]
        }
      ],
      "source": [
        "print('Dimensions of train dataset : ', train_data.shape)\n",
        "print('Dimensions of test dataset : ', test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM5_LNWP2iu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d2ea38-fc65-49d2-e43e-f2d42792bd3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of unique users:  122565\n",
            "the number of unique movies:  15882\n",
            "number of ratings:  2097150\n",
            "the full rating matrix should have:  1946577330 elements\n",
            "0.1077352524186645 % of the matrix is filled\n"
          ]
        }
      ],
      "source": [
        "#diagnostics\n",
        "combined_df = pd.concat([train_data, test_data],ignore_index=True)\n",
        "\n",
        "num_users_combined = len(combined_df.userID.unique())\n",
        "print(\"the number of unique users: \", num_users_combined)\n",
        "\n",
        "num_movies_combined = len(combined_df.movieID.unique())\n",
        "print(\"the number of unique movies: \", num_movies_combined)\n",
        "\n",
        "num_ratings = len(combined_df.rating)\n",
        "print(\"number of ratings: \", num_ratings)\n",
        "\n",
        "num_elements = num_users_combined * num_movies_combined\n",
        "print(\"the full rating matrix should have: \",num_elements,\"elements\")\n",
        "\n",
        "percentage_filled = (num_ratings / num_elements) * 100\n",
        "print(percentage_filled, \"% of the matrix is filled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2AmYgUw2nWK"
      },
      "source": [
        "In order to address the issue of high sparsity (our matrix is only 0.108% filled), we have to perform matrix factorisation\n",
        "- The purpose is to estimate or predict the missing values in a sparse user-item matrix\n",
        "- We do this by decomposing the large user-interaction matrix, R, into 2 lower-dimensional matrices (called embeddings) for users and items\n",
        "- The product of these 2 submatrices is our approximation of R, which we can then use in our recommender system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeT2BEWm2n3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0076c1-2bee-47aa-8c65-22a6c2686202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.17207838 0.42534637 0.65839395 ... 0.71906646 0.740008   0.59971561]\n",
            " [0.30066043 0.18271293 0.2342668  ... 0.43681656 0.45394934 0.75707883]\n",
            " [0.10661033 0.41688034 0.35936381 ... 0.69983677 0.66405643 0.7809003 ]\n",
            " ...\n",
            " [0.28409663 0.60286212 0.43296182 ... 0.42267257 0.06179583 0.89620821]\n",
            " [0.7094807  0.7603623  0.13455213 ... 0.09779999 0.09691884 0.80434778]\n",
            " [0.7164048  0.00929157 0.46637734 ... 0.6412743  0.27761569 0.52840397]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# number of latent features\n",
        "latent_features = 10\n",
        "\n",
        "# number of unique users in the training dataset\n",
        "num_users_training = len(train_data.userID.unique())\n",
        "# number of unique items in the traning dataset\n",
        "num_items_training = len(train_data.movieID.unique())\n",
        "\n",
        "# creating the actual matrix with 0 as placeholder for non-rated movies\n",
        "user_item_matrix = train_data.pivot(index='userID',columns='movieID',values='rating').fillna(0)\n",
        "# print(user_item_matrix)\n",
        "\n",
        "# user matrix\n",
        "user_matrix = np.random.rand(num_users_training, latent_features)\n",
        "print(user_matrix)\n",
        "\n",
        "# item matrix\n",
        "item_matrix = np.random.rand(num_items_training, latent_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWE6iCao4hbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e65335-d6fe-428a-d3a7-8bbfaf48b28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movieID  329    1798   2400   8651   10341  10774  12779  13793  14660  15381\n",
            "userID                                                                       \n",
            "377114     0.0    0.0    0.0    0.0    0.0    0.0    2.0    0.0    0.0    0.0\n",
            "510180     0.0    5.0    0.0    2.0    4.0    3.0    0.0    0.0    2.0    0.0\n",
            "680148     0.0    0.0    4.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
            "1192119    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    4.0\n",
            "1788180    5.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
            "2582768    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0\n",
            "\n",
            "\n",
            "[[0.97046727 0.87969962 0.43768047 0.90677333 0.79388762 0.20208995\n",
            "  0.56778444 0.79197428 0.26369141 0.81666452]\n",
            " [0.51748121 0.18256488 0.36460127 0.79201763 0.13713189 0.17339275\n",
            "  0.97686148 0.61063822 0.40581756 0.6808908 ]\n",
            " [0.44054541 0.60638376 0.2746372  0.1393762  0.23793504 0.57387056\n",
            "  0.57284267 0.02913906 0.91922212 0.61174861]\n",
            " [0.04246027 0.36219835 0.97693579 0.23297703 0.25487773 0.88485233\n",
            "  0.57638523 0.94014214 0.41393384 0.99991424]\n",
            " [0.58009993 0.70640509 0.48551052 0.73678255 0.46083612 0.99479931\n",
            "  0.39487523 0.62306415 0.09584214 0.83161182]\n",
            " [0.86779739 0.70489619 0.89480564 0.87608304 0.27324115 0.85806434\n",
            "  0.92214606 0.39476012 0.43442794 0.19092951]]\n",
            "\n",
            "\n",
            "[[0.94023634 0.76180099 0.0502452  0.65823339 0.19500656 0.85306285\n",
            "  0.281043   0.11238045 0.54939512 0.49621315]\n",
            " [0.63323939 0.82588354 0.12412522 0.26571544 0.90038003 0.39536804\n",
            "  0.32462113 0.90982377 0.47955163 0.14554582]\n",
            " [0.10916692 0.22349685 0.14577722 0.24982891 0.53745618 0.23761244\n",
            "  0.93988183 0.46674732 0.18870219 0.41680592]\n",
            " [0.15869524 0.8195554  0.70144601 0.66809229 0.20251342 0.61151989\n",
            "  0.83488542 0.968486   0.27968437 0.87300277]\n",
            " [0.07491829 0.03312557 0.02430138 0.76279185 0.75805298 0.59547806\n",
            "  0.19828306 0.33035744 0.92449418 0.1220519 ]\n",
            " [0.66391466 0.34073894 0.60763446 0.62503335 0.57384215 0.12814705\n",
            "  0.67998137 0.69837028 0.35997093 0.16111059]\n",
            " [0.65647853 0.89339412 0.51595243 0.15607385 0.57195097 0.51075704\n",
            "  0.57817158 0.58480212 0.53625993 0.14780519]\n",
            " [0.09492535 0.96155138 0.3180479  0.16984524 0.49300979 0.05270682\n",
            "  0.86255149 0.7620146  0.94125755 0.20743434]\n",
            " [0.66888485 0.03964319 0.80862233 0.46207301 0.33412    0.81843245\n",
            "  0.78350379 0.47334665 0.5227751  0.85497104]\n",
            " [0.70855165 0.2954569  0.74363403 0.73674375 0.99743209 0.36623459\n",
            "  0.4462754  0.7037672  0.7130431  0.32108483]]\n",
            "\n",
            "\n",
            "[[0.94023634 0.63323939 0.10916692 0.15869524 0.07491829 0.66391466\n",
            "  0.65647853 0.09492535 0.66888485 0.70855165]\n",
            " [0.76180099 0.82588354 0.22349685 0.8195554  0.03312557 0.34073894\n",
            "  0.89339412 0.96155138 0.03964319 0.2954569 ]\n",
            " [0.0502452  0.12412522 0.14577722 0.70144601 0.02430138 0.60763446\n",
            "  0.51595243 0.3180479  0.80862233 0.74363403]\n",
            " [0.65823339 0.26571544 0.24982891 0.66809229 0.76279185 0.62503335\n",
            "  0.15607385 0.16984524 0.46207301 0.73674375]\n",
            " [0.19500656 0.90038003 0.53745618 0.20251342 0.75805298 0.57384215\n",
            "  0.57195097 0.49300979 0.33412    0.99743209]\n",
            " [0.85306285 0.39536804 0.23761244 0.61151989 0.59547806 0.12814705\n",
            "  0.51075704 0.05270682 0.81843245 0.36623459]\n",
            " [0.281043   0.32462113 0.93988183 0.83488542 0.19828306 0.67998137\n",
            "  0.57817158 0.86255149 0.78350379 0.4462754 ]\n",
            " [0.11238045 0.90982377 0.46674732 0.968486   0.33035744 0.69837028\n",
            "  0.58480212 0.7620146  0.47334665 0.7037672 ]\n",
            " [0.54939512 0.47955163 0.18870219 0.27968437 0.92449418 0.35997093\n",
            "  0.53625993 0.94125755 0.5227751  0.7130431 ]\n",
            " [0.49621315 0.14554582 0.41680592 0.87300277 0.1220519  0.16111059\n",
            "  0.14780519 0.20743434 0.85497104 0.32108483]]\n"
          ]
        }
      ],
      "source": [
        "# smaller (sample) data for checking\n",
        "smaller_data = pd.concat([train_data.head(),train_data.tail()], ignore_index=True)\n",
        "# print(smaller_data)\n",
        "\n",
        "# smaller matrix\n",
        "smaller_matrix = smaller_data.pivot(index = 'userID',columns = 'movieID',values = 'rating').fillna(0)\n",
        "print(smaller_matrix)\n",
        "print(\"\\n\")\n",
        "\n",
        "# embedding for users\n",
        "smaller_user_matrix = np.random.rand(6, latent_features)\n",
        "print(smaller_user_matrix)\n",
        "print(\"\\n\")\n",
        "\n",
        "# embedding for items\n",
        "smaller_item_matrix = np.random.rand(10, latent_features)\n",
        "print(smaller_item_matrix)\n",
        "print(\"\\n\")\n",
        "print(smaller_item_matrix.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMjAkpUr4jtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dc8ba5-e2ec-4574-e92c-b04ab1e3cd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movieID  329    1798   2400   8651   10341  10774  12779  13793  14660  15381\n",
            "userID                                                                       \n",
            "377114     0.0    0.0    0.0    0.0    0.0    0.0    2.0    0.0    0.0    0.0\n",
            "\n",
            "\n",
            "movieID\n",
            "329      0.0\n",
            "1798     0.0\n",
            "2400     0.0\n",
            "8651     0.0\n",
            "10341    0.0\n",
            "10774    0.0\n",
            "12779    2.0\n",
            "13793    0.0\n",
            "14660    0.0\n",
            "15381    0.0\n",
            "Name: 377114, dtype: float64\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# how to access a single row in the matrix (horizontally)\n",
        "firstrow = smaller_matrix.iloc[[0]]\n",
        "print(firstrow)\n",
        "print(\"\\n\")\n",
        "\n",
        "# how to access a single row in the matrix (vertically)\n",
        "firstrow = smaller_matrix.iloc[0]\n",
        "print(firstrow)\n",
        "print(\"\\n\")\n",
        "\n",
        "# gets the ratings for each of the movies from eacha nd every users\n",
        "# for item in firstrow.index:\n",
        "        # rating = smaller_matrix[item]\n",
        "        # print(rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the working principle of the nn.Embedding method to understand how it works and how to incorparet in collaborative filtering"
      ],
      "metadata": {
        "id": "r3qnxbcx-mBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# number of unique users in the training dataset\n",
        "num_users_training = len(train_data.userID.unique())\n",
        "# number of unique items in the traning dataset\n",
        "num_items_training = len(train_data.movieID.unique())\n",
        "\n",
        "\n",
        "\n",
        "# testing\n",
        "# this part to be done after matrix factorization (to get optimal latent feature values. now still randomized)\n",
        "\n",
        "# user embeddings\n",
        "user_embedding = nn.Embedding(num_users_training, latent_features)\n",
        "user_indices = torch.LongTensor([1,2,3]) # tensor containing user indices\n",
        "user_embeds = user_embedding(user_indices)\n",
        "print(user_embeds)\n",
        "print(\"\\n\")\n",
        "\n",
        "# item embeddings\n",
        "item_embedding = nn.Embedding(num_items_training, latent_features)\n",
        "item_indices = torch.LongTensor([1,2,3]) # tensor containing item indices\n",
        "item_embeds = item_embedding(item_indices)\n",
        "print(item_embeds)\n",
        "print(\"\\n\")\n",
        "\n",
        "all_user_item_pairs = torch.cartesian_prod(user_indices, item_indices)\n",
        "# print(all_user_item_pairs)\n",
        "\n",
        "all_user_embeds = user_embedding(all_user_item_pairs[:, 0])\n",
        "all_item_embeds = item_embedding(all_user_item_pairs[:, 1])\n",
        "\n",
        "# Concatenate user and item embeddings for each user-item pair\n",
        "user_item_embeds = torch.cat((all_user_embeds, all_item_embeds), dim=1)\n",
        "\n",
        "print(user_item_embeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lH9h-ndrBYL",
        "outputId": "7ec85a32-b954-4cd3-cd8a-c2263a33c23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0364,  1.0254, -1.6430,  0.6978,  0.1644, -2.1614,  0.9825,  0.9546,\n",
            "         -0.5878,  0.4509],\n",
            "        [-0.4772,  0.2392,  1.3269, -0.8494,  1.0262, -0.2117, -0.6728,  0.6724,\n",
            "          2.1308, -2.0784],\n",
            "        [ 2.1504, -0.0107,  1.9343,  0.7831,  1.1436, -0.1862, -0.0571, -1.2564,\n",
            "          0.6780,  1.6813]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "\n",
            "tensor([[-0.2547, -0.5119, -1.0710,  0.2244, -1.2941, -0.2673,  0.2925,  0.6664,\n",
            "          0.1585,  1.3610],\n",
            "        [ 1.4909,  1.3809, -0.4581, -1.9810, -0.3197,  0.9922,  0.6793, -1.1011,\n",
            "          0.6199,  1.5650],\n",
            "        [-0.1942, -1.1281,  0.8591, -0.5086,  0.8517, -0.2058, -0.1262, -0.8411,\n",
            "         -0.0047, -0.4126]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "\n",
            "tensor([[-0.0364,  1.0254, -1.6430,  0.6978,  0.1644, -2.1614,  0.9825,  0.9546,\n",
            "         -0.5878,  0.4509, -0.2547, -0.5119, -1.0710,  0.2244, -1.2941, -0.2673,\n",
            "          0.2925,  0.6664,  0.1585,  1.3610],\n",
            "        [-0.0364,  1.0254, -1.6430,  0.6978,  0.1644, -2.1614,  0.9825,  0.9546,\n",
            "         -0.5878,  0.4509,  1.4909,  1.3809, -0.4581, -1.9810, -0.3197,  0.9922,\n",
            "          0.6793, -1.1011,  0.6199,  1.5650],\n",
            "        [-0.0364,  1.0254, -1.6430,  0.6978,  0.1644, -2.1614,  0.9825,  0.9546,\n",
            "         -0.5878,  0.4509, -0.1942, -1.1281,  0.8591, -0.5086,  0.8517, -0.2058,\n",
            "         -0.1262, -0.8411, -0.0047, -0.4126],\n",
            "        [-0.4772,  0.2392,  1.3269, -0.8494,  1.0262, -0.2117, -0.6728,  0.6724,\n",
            "          2.1308, -2.0784, -0.2547, -0.5119, -1.0710,  0.2244, -1.2941, -0.2673,\n",
            "          0.2925,  0.6664,  0.1585,  1.3610],\n",
            "        [-0.4772,  0.2392,  1.3269, -0.8494,  1.0262, -0.2117, -0.6728,  0.6724,\n",
            "          2.1308, -2.0784,  1.4909,  1.3809, -0.4581, -1.9810, -0.3197,  0.9922,\n",
            "          0.6793, -1.1011,  0.6199,  1.5650],\n",
            "        [-0.4772,  0.2392,  1.3269, -0.8494,  1.0262, -0.2117, -0.6728,  0.6724,\n",
            "          2.1308, -2.0784, -0.1942, -1.1281,  0.8591, -0.5086,  0.8517, -0.2058,\n",
            "         -0.1262, -0.8411, -0.0047, -0.4126],\n",
            "        [ 2.1504, -0.0107,  1.9343,  0.7831,  1.1436, -0.1862, -0.0571, -1.2564,\n",
            "          0.6780,  1.6813, -0.2547, -0.5119, -1.0710,  0.2244, -1.2941, -0.2673,\n",
            "          0.2925,  0.6664,  0.1585,  1.3610],\n",
            "        [ 2.1504, -0.0107,  1.9343,  0.7831,  1.1436, -0.1862, -0.0571, -1.2564,\n",
            "          0.6780,  1.6813,  1.4909,  1.3809, -0.4581, -1.9810, -0.3197,  0.9922,\n",
            "          0.6793, -1.1011,  0.6199,  1.5650],\n",
            "        [ 2.1504, -0.0107,  1.9343,  0.7831,  1.1436, -0.1862, -0.0571, -1.2564,\n",
            "          0.6780,  1.6813, -0.1942, -1.1281,  0.8591, -0.5086,  0.8517, -0.2058,\n",
            "         -0.1262, -0.8411, -0.0047, -0.4126]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "# performing matrix multiplication\n",
        "mf_vector = torch.mm(user_embeds, item_embeds.T)\n",
        "print(mf_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zzKYyG8IEgd",
        "outputId": "77fae3a2-99d7-4ebb-82f6-d05a1457154b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3.2096, -1.5073, -3.4416],\n",
            "        [-5.1236, -2.9736,  2.6791],\n",
            "        [-2.3265,  4.5993,  2.2374]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# the user embedding layer maps user indices to user embeddings\n",
        "user_embedding = nn.Embedding(num_users_training, latent_features)\n",
        "print(user_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btl5tpCoJpzR",
        "outputId": "27a98fd7-6099-4b94-c75c-39540108c828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(9174, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# testing (matrix multiplication)\n",
        "user_embedding_mf = nn.Embedding(num_users_training, latent_features)\n",
        "item_embedding_mf = nn.Embedding(num_items_training, latent_features)\n",
        "\n",
        "\n",
        "print(user_embedding_mf)\n",
        "\n",
        "user_embeds = user_embedding_mf()\n",
        "\n",
        "# Access the learned embeddings\n",
        "user_embeddings = user_embedding_mf.weight.data\n",
        "\n",
        "# Print the embeddings\n",
        "print(\"User Embeddings:\")\n",
        "print(user_embeddings)\n",
        "\n",
        "\n",
        "# print(num_users_training)\n",
        "# print(latent_features)\n",
        "# mf_vector = nn.Dropout(config.dropout_rate_mf)(mf_vector)"
      ],
      "metadata": {
        "id": "bBnYQ4fk33ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset is an abstract class\n",
        "\n",
        "# loading data, preprocessing,\n",
        "class Loader(Dataset):\n",
        "\n",
        "  def __init__(self, users, items, ratings):\n",
        "    self.users = torch.tensor(users, dtype=torch.long)\n",
        "    self.items = torch.tensor(items, dtype=torch.long)\n",
        "    self.ratings = torch.tensor(ratings, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.users)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        'users': self.users[idx]\n",
        "        'items': self.items[idx]\n",
        "        'ratings': self.ratings[idx]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "J3ga90QNrZuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# number of unique users in the training dataset\n",
        "num_users_training = len(train_data.userID.unique())\n",
        "# number of unique items in the traning dataset\n",
        "num_items_training = len(train_data.movieID.unique())\n",
        "\n",
        "class MatrixFactorization(nn.Module):\n",
        "\n",
        "  # initializing attributes\n",
        "  def __init__(self, num_users_training, num_items_training, latent_features=10):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # creating user and item embedding layers for matrix factorization\n",
        "    self.user_embedding = nn.Embedding(num_users_training, latent_features)\n",
        "    self.item_embedding = nn.Embedding(num_items_training, latent_features)\n",
        "\n",
        "  # Define a matrix factorization model class using PyTorch's nn.Module.\n",
        "  # The forward pass returns the dot product of user and item embeddings, summing over latent features.\n",
        "  def forward(self, user, item):\n",
        "    return(self.user_embedding(user) * self.item_embedding(item)).sum(1)\n",
        "\n",
        "\n",
        "    # leaky relu activation function\n",
        "    # self.relu = nn.LeakyReLU()\n",
        "\n",
        "    # container to store all the layers of the neural network\n",
        "    # self.layer_container = nn.ModuleList()\n",
        "\n",
        "    # initializing the weights of the user and item embedding layers with values sampled from a uniform distribution\n",
        "    # these values are between 0 and 0.05. Since a normal distribution is used, most values would centre around the mean\n",
        "    # we just assign an arbitrary value for the weights at the start, and the values of the weights will be tweaked when we train our model\n",
        "    # self.user_embedding.weight.data.uniform(0, 0.05)\n",
        "    # self.item_embedding.weight.data.uniform(0, 0.05)"
      ],
      "metadata": {
        "id": "VOTMpccYyidh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # when training, call this function (it specifies how input data is processed through the layers of the network to produce an output)\n",
        "    def forward(self, user_indices, item_indices):\n",
        "\n",
        "      user_embeds = self.user_embedding(user_indices)\n",
        "      item_embeds = self.item_embedding(item_indices)\n",
        "      x = torch.cat([user_embeds, item_embeds], dim=1)\n",
        "\n",
        "      x = self.fc1(x) # this calculates the weighted sum of all the neurons in the first layer, while taking into account their biases\n",
        "      x = self.relu(x) # applying the leaky relu activation function to the weighted sum\n",
        "      x = self.fc2(x) # applies another linear transformation\n",
        "      return x"
      ],
      "metadata": {
        "id": "JCq262v03fa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# number of unique users in the training dataset\n",
        "num_users_training = len(train_data.userID.unique())\n",
        "# number of unique items in the traning dataset\n",
        "num_items_training = len(train_data.movieID.unique())\n",
        "\n",
        "\n",
        "# testing\n",
        "# this part to be done after matrix factorization (to get optimal latent feature values. now still randomized)\n",
        "\n",
        "# user embeddings\n",
        "user_embedding = nn.Embedding(num_users_training, latent_features)\n",
        "user_indices = torch.LongTensor([1,2,3]) # tensor containing user indices\n",
        "user_embeds = user_embedding(user_indices)\n",
        "print(user_embeds)\n",
        "print(\"\\n\")\n",
        "\n",
        "# item embeddings\n",
        "item_embedding = nn.Embedding(num_items_training, latent_features)\n",
        "item_indices = torch.LongTensor([1,2,3]) # tensor containing item indices\n",
        "item_embeds = item_embedding(item_indices)\n",
        "print(item_embeds)\n",
        "print(\"\\n\")\n",
        "\n",
        "all_user_item_pairs = torch.cartesian_prod(user_indices, item_indices)\n",
        "# print(all_user_item_pairs)\n",
        "\n",
        "all_user_embeds = user_embedding(all_user_item_pairs[:, 0])\n",
        "all_item_embeds = item_embedding(all_user_item_pairs[:, 1])\n",
        "\n",
        "# Concatenate user and item embeddings for each user-item pair\n",
        "user_item_embeds = torch.cat((all_user_embeds, all_item_embeds), dim=1)\n",
        "\n",
        "# print(user_item_embeds)\n",
        "mf_vector = torch.mul(user_embeds, item_embeds)\n",
        "print(mf_vector)\n",
        "\n",
        "mf_vector = torch.mm(user_embeds, item_embeds.T)\n",
        "print(mf_vector)\n",
        "\n"
      ],
      "metadata": {
        "id": "gv2J0RzTOzK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# testing (matrix multiplication)\n",
        "user_embedding_mf = nn.Embedding(num_users_training, latent_features)\n",
        "item_embedding_mf = nn.Embedding(num_items_training, latent_features)\n",
        "print(latent_features)\n",
        "print(user_embedding_mf)\n",
        "print(item_embedding_mf)\n",
        "# Access the learned embeddings\n",
        "user_embeddings = user_embedding_mf.weight.data\n",
        "\n",
        "# Print the embeddings\n",
        "print(\"User Embeddings:\")\n",
        "print(user_embeddings)\n",
        "\n",
        "# Forward with a single user index\n",
        "user_index = torch.tensor([9173])\n",
        "user_embeds = user_embedding_mf(user_index)\n",
        "print(\"User Embedding for index 377114:\")\n",
        "print(user_embeds)\n",
        "\n",
        "item_index = torch.tensor([4632])\n",
        "item_embeds = item_embedding_mf(item_index)\n",
        "print(\"Item Embedding for index 9173:\")\n",
        "print(item_embeds)\n",
        "\n",
        "# Concatenate user and item embeddings\n",
        "combined_embeds = torch.cat([user_embeds, item_embeds], dim=1)\n",
        "print(\"Combined Embeddings:\")\n",
        "print(combined_embeds)\n",
        "\n",
        "# Linear transformation\n",
        "linear_layer = nn.Linear(latent_features * 2, 1)\n",
        "print(linear_layer)\n",
        "\n",
        "# Access layer parameters\n",
        "print(\"Weight matrix shape:\", linear_layer.weight.shape)\n",
        "print(\"Bias vector shape:\", linear_layer.bias.shape)\n",
        "\n",
        "# Apply linear layer to the combined embeddings\n",
        "output = linear_layer(combined_embeds)\n",
        "print(\"Output after Linear Transformation:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "YHA0xcLLGX4G",
        "outputId": "3b5a99d8-f783-4754-c49e-a2681c0ed5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "Embedding(9174, 10)\n",
            "Embedding(4633, 10)\n",
            "User Embeddings:\n",
            "tensor([[-0.7093, -1.4774,  0.7135,  ..., -1.1078,  1.1718, -0.4468],\n",
            "        [-0.2315, -1.8767, -0.3631,  ..., -0.9548,  0.6520,  1.3159],\n",
            "        [ 0.2791, -0.2546,  1.4355,  ..., -1.1245, -1.1717,  0.7856],\n",
            "        ...,\n",
            "        [-0.0841, -0.6173, -0.5962,  ..., -1.7418, -0.4762, -0.1816],\n",
            "        [-2.9315,  0.2006, -0.6173,  ..., -0.4202, -0.7122, -0.3279],\n",
            "        [ 0.2435,  0.5883, -0.7190,  ..., -1.2386, -0.4424,  0.1986]])\n",
            "User Embedding for index 377114:\n",
            "tensor([[ 0.2435,  0.5883, -0.7190, -0.7269, -0.3127, -0.2542, -0.9676, -1.2386,\n",
            "         -0.4424,  0.1986]], grad_fn=<EmbeddingBackward0>)\n",
            "Item Embedding for index 9173:\n",
            "tensor([[ 0.1444,  0.8047,  0.0154,  1.3510,  1.8176,  0.7132, -0.8718,  1.0490,\n",
            "         -1.3116,  0.3669]], grad_fn=<EmbeddingBackward0>)\n",
            "Combined Embeddings:\n",
            "tensor([[ 0.2435,  0.5883, -0.7190, -0.7269, -0.3127, -0.2542, -0.9676, -1.2386,\n",
            "         -0.4424,  0.1986,  0.1444,  0.8047,  0.0154,  1.3510,  1.8176,  0.7132,\n",
            "         -0.8718,  1.0490, -1.3116,  0.3669]], grad_fn=<CatBackward0>)\n",
            "Linear(in_features=20, out_features=1, bias=True)\n",
            "Weight matrix shape: torch.Size([1, 20])\n",
            "Bias vector shape: torch.Size([1])\n",
            "Output after Linear Transformation:\n",
            "tensor([[0.5993]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FLi4OJbTO0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JBm-LmgTO3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hurBoWinomnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T1LlgpWAomuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataset provides a standard interface for accessing and working with data in PyTorch\n",
        "# DataLoader depends on Dataset instances for its operation.\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn import model_selection, preprocessing, mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Data Preprocessing\n",
        "lbl_user = preprocessing.LabelEncoder()\n",
        "lbl_movie = preprocessing.LabelEncoder()\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df['userId'] = lbl_user.fit_transform(df['userId'].values)\n",
        "df['movieId'] = lbl_movie.fit_transform(df['movieId'].values)\n",
        "\n",
        "# Dataset Definition\n",
        "# data loader class\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, users, movies, ratings):\n",
        "        self.users = torch.tensor(users, dtype=torch.long)\n",
        "        self.movies = torch.tensor(movies, dtype=torch.long)\n",
        "        self.ratings = torch.tensor(ratings, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'users': self.users[idx],\n",
        "            'movies': self.movies[idx],\n",
        "            'ratings': self.ratings[idx]\n",
        "                }\n",
        "\n",
        "# DataLoader Setup\n",
        "# DataLoader is to help efficiently load and iterate over datasets during the training or evaluation\n",
        "df_train, df_valid = model_selection.train_test_split(\n",
        "    df, test_size=0.1, random_state=42, stratify=df['rating'].values\n",
        ")\n",
        "\n",
        "train_dataset = MovieDataset(\n",
        "    users=df_train['userId'].values,\n",
        "    movies=df_train['movieId'].values,\n",
        "    ratings=df_train['rating'].values\n",
        ")\n",
        "\n",
        "valid_dataset = MovieDataset(\n",
        "    users=df_valid['userId'].values,\n",
        "    movies=df_valid['movieId'].values,\n",
        "    ratings=df_valid['rating'].values\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "validation_loader = DataLoader(dataset=valid_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Model Definition\n",
        "# inherits from the nn.Module class\n",
        "class RecSysModel(nn.Module):\n",
        "    def __init__(self, n_users, n_movies):\n",
        "        super(RecSysModel, self).__init__()\n",
        "        self.user_embed = nn.Embedding(n_users, 32)\n",
        "        self.movie_embed = nn.Embedding(n_movies, 32)\n",
        "        # This is a linear layer that takes the concatenated user adn movie embeddings as input (total input size is 32+32=64) and produces a single rating prediction using a linear transformation\n",
        "        self.out = nn.Linear(64, 1)\n",
        "\n",
        "    # computes predictions based on user and item embeddings\n",
        "    def forward(self, users, movies):\n",
        "        user_embeds = self.user_embed(users)\n",
        "        movie_embeds = self.movie_embed(movies)\n",
        "        output = torch.cat([user_embeds, movie_embeds], dim=1)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "\n",
        "# Model Initialization\n",
        "model = RecSysModel(\n",
        "    n_users=len(lbl_user.classes_),\n",
        "    n_movies=len(lbl_movie.classes__),\n",
        ").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Scheduler\n",
        "# The learning rate scheduler adjusts the learning rate of the optimizer during training. It will decrease the learning rate by a factor of gamma (0.7) every step_size (3) epochs.\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.7)\n",
        "\n",
        "# Loss Function that will be used is a mean-squared error\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "epochs = 1\n",
        "total_loss = 0 # tracks progress and overall loss\n",
        "plot_steps = 5000\n",
        "print_steps = 5000\n",
        "step_cnt = 0 # number of samples processed (number of batches * 4)\n",
        "all_losses_list = []\n",
        "\n",
        "# sets the model to training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for i, train_data in enumerate(train_loader): # train_loader is a DataLoader object that provides the batches of training data (users,movies,ratings) to the model\n",
        "        output = model(train_data[\"users\"], train_data[\"movies\"])\n",
        "        rating = train_data[\"ratings\"].view(4, -1).to(torch.float32)\n",
        "        loss = loss_func(output, rating) # computes the loss (error) between the predicted rating and the actual rating\n",
        "        total_loss += loss.sum().item()\n",
        "\n",
        "        # backward pass and parameter update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        step_cnt += len(train_data[\"users\"])\n",
        "\n",
        "        # prints out the loss after\n",
        "        if step_cnt % plot_steps == 0:\n",
        "            avg_loss = total_loss / (len(train_data[\"users\"]) * plot_steps)\n",
        "            print(f\"epoch {epoch} loss at step: {step_cnt} is {avg_loss}\")\n",
        "            all_losses_list.append(avg_loss)\n",
        "            total_loss = 0\n",
        "\n",
        "model_output_list = [] # predicted rating list\n",
        "target_rating_list = [] # actual rating list\n",
        "\n",
        "model.eval()  # sets model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batched_data in enumerate(train_loader):\n",
        "        model_output = model(batched_data['users'], batched_data['movies'])\n",
        "        model_output_list.append(model_output.sum().item() / len(batched_data['users']))\n",
        "\n",
        "        target_rating = batched_data['ratings']\n",
        "        target_rating_list.append(target_rating.sum().item() / len(batched_data['users']))\n",
        "\n",
        "        print(f\"model_output: {model_output}, target_rating: {target_rating}\")\n",
        "\n",
        "# If True returns MSE value, if False returns RMSE value.\n",
        "rms = mean_squared_error(target_rating_list, model_output_list, squared=False)\n",
        "print(f\"rms: {rms}\")"
      ],
      "metadata": {
        "id": "yJr_pWM3om1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}